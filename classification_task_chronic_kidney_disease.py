# -*- coding: utf-8 -*-
"""Classification task - chronic kidney disease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lFs6uaq_cDPhJ0XKXAVAC7MxQsoxsPrQ

# **The flow to classify a chronic kidney disease**
"""

import numpy as np #computations
import pandas as pd
import os, sys #system related settings
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
import seaborn as sns # beautiful visualizations
sns.set()

df = pd.read_csv('/content/kidney_disease.csv')
df.head(n=10)

df.shape

# check for null
df.isnull().sum()

# Imputing Null values

from sklearn.impute import SimpleImputer
imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

df_imputed = pd.DataFrame(imp_mode.fit_transform(df))
df_imputed.columns = df.columns
df_imputed

df_imputed.isnull().sum()

"""# **Finding unique values in the columns**"""

df_imputed.columns

# helps to check for wrong values
for i in df_imputed.columns:
  print("***************************************************************************", i, "*************************************************************")
  print()
  print(set(df_imputed[i].tolist()))
  print()

"""Getting wrongs: 'ckd\t', '\tno', ' yes', '\t43' and etc

For numerical: finding the mode values (the most common) to replace invalid values with.
"""

print(df_imputed['rc'].mode())
print(df_imputed['wc'].mode())
print(df_imputed['pcv'].mode())

"""For literals: checking for wrong values and replacing with correct ones."""

# replacing
df_imputed['classification']=df_imputed['classification'].apply(lambda x: 'ckd' if x=='ckd\t' else x)

# replacing ...?
df_imputed['cad']=df_imputed['cad'].apply(lambda x: 'no' if x=='\tno' else x)

df_imputed['dm']=df_imputed['dm'].apply(lambda x: 'no' if x=='\tno' else x)
df_imputed['dm']=df_imputed['dm'].apply(lambda x: 'yes' if x=='\tyes' else x)
df_imputed['dm']=df_imputed['dm'].apply(lambda x: 'yes' if x==' yes' else x)

df_imputed['rc']=df_imputed['rc'].apply(lambda x: '5.2' if x=='\t?' else x)

df_imputed['wc']=df_imputed['wc'].apply(lambda x: '9800' if x=='\t6200' else x)
df_imputed['wc']=df_imputed['wc'].apply(lambda x: '9800' if x=='\t8400' else x)
df_imputed['wc']=df_imputed['wc'].apply(lambda x: '9800' if x=='\t?' else x)

df_imputed['pcv']=df_imputed['pcv'].apply(lambda x: '41' if x=='\t43' else x)
df_imputed['pcv']=df_imputed['pcv'].apply(lambda x: '41' if x=='\t?' else x)

"""# **Checking label imbalance**"""

import matplotlib.pyplot as plt
import seaborn as sns

temp=df_imputed['classification'].value_counts()
temp_df=pd.DataFrame({'classification': temp.index, 'values': temp.values})
print(sns.barplot(x='classification', y='values', data=temp_df))

df.dtypes

df_imputed.dtypes

# helps to check for wrong values
for i in df_imputed.columns:
  print("***************************************************************************", i, "*************************************************************")
  print()
  print(set(df_imputed[i].tolist()))
  print()

"""Big difference in values' amount. Needs to be fixed."""

# fixing data types
# But! After imputing we lose the right meaning of data:
df_imputed.dtypes

"""To fix this, from the main df exclude objects:"""

# exclude objects and keep only numericals
for i in df.select_dtypes(exclude=['object']).columns:
  df_imputed[i]=df_imputed[i].apply(lambda x: float(x))

"""# **Visualisation**"""

sns.pairplot(df_imputed, hue='classification')

"""Find the distribution of all numerical columns"""

def displots(col):
  sns.displot(df[col])
  plt.show()

for i in list(df_imputed.select_dtypes(exclude=['object']).columns)[1:]:
  displots(i)

"""The data is mostly normally distributed

Outliers of data
"""

# Finding and remove outliers
def boxplots(col):
  # Ensures vertical orientation
  sns.boxplot(x=df[col])
  plt.show()

for i in list(df_imputed.select_dtypes(exclude=['object']).columns)[1:]:
  boxplots(i)

"""The outliers seen here are actually possible values. Have seen it with medical professionals

Final step of the data preprocessing is label encoding (tp convert categorical data into numerical).
Encode the data is needed because there are categorical and numerical columns, but algorytms expect only numerical input. Many ways of this are available. Currently label encoding is used.
"""

# Label encoding to convert categorical values to numerical
from sklearn import preprocessing

df_encoded=df_imputed.apply(preprocessing.LabelEncoder().fit_transform)
df_encoded

"""Save the preprocessed data"""

df_encoded.to_csv('Kidney_Disease_Pre-processed_Data.csv')

"""# **Correlation**

Finding correlation between independent variables important, becasue we dont want to fit the wrong data to a model. If there is such a corellation, the value of the weight would be altered, and the output would not be right.
"""

# Find correlation

plt.figure(figsize=(20,20))
corr=df_encoded.corr()
sns.heatmap(corr, annot=True)

# Separate independent and dependent variables and drop the ID col.
x=df_encoded.drop(['id', 'classification'], axis=1)
y=df_encoded['classification'] # dependent

from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

print(Counter(y))

ros = RandomOverSampler(random_state=42)
x_ros, y_ros = ros.fit_resample(x, y)
print(Counter(y_ros))

"""# **Scaling**

Scaling is especially important in the algorytms where the distace between the data point is important, such as SVM and KNN
"""

# Initializing MinMaxScaler and scale the features to between -1 and 1 to normalize them.
scaler=MinMaxScaler(feature_range=(-1,1))
# the fit_transform() method fits to the data and then transforms it. No need to scale labels
x=scaler.fit_transform(x_ros)
y=y_ros

"""# **Feature extraction or dimensionality reduction**

Reduce the dimensionallity of the independent variables.
"""

# Applying PCA

# The code below has .95 for the number of components parameter.
# It means that scikit-learn choose the minimum number of principal components such that 95% of the variance is retained.

import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls
from sklearn.decomposition import PCA

pca = PCA(.95)
X_PCA=pca.fit_transform(x)

print(x.shape)
print(X_PCA.shape)

# Thus we need 18 columns to keep 95% of the variance

# with PCA
# spliting the data into training and testing datasets keeping 20% for testing
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X_PCA, y, test_size=0.2, random_state=7)

"""# **Creating neural network**
(and then fitting the data)
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install tensorflow

import tensorflow.keras

# A Sequential model is appropriate for a plane stack of layers, where each layer has exactly one input tensor and one output tensor. Straight up connection between layers. No feedback and etc comming back from those layers.
from keras.models import Sequential

# Dense layer is the regular deeply connected neural network layer. Most common and used.. No feedback as well.
from keras.layers import Dense

# the Droput layer randomly sets input units to 0 with a frequency of rate at each step during trainig time, which helps prevent overfitting
from keras.layers import Dropout

# saves a certain point of a trained model to continue witj
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.models import Sequential, Model

# Optimizers are algoritms or methods used to change the attributes of the NN sich as weight and learning rate to reduce the losses. Optimizers are used to solve optimization problems by minimazing the function.
# Adam optimization is an extension to ''Stochastic gradient descend'' and can be used in place of classical stochastic gradient descent to update network weights more efficiently.
from keras.optimizers import Adam
from sklearn.model_selection import KFold # not used here since no Rule-based algoritms

# creating the model

def model():
  # architecture:
  classifier = Sequential()
            # depth - randomly chosen:    # number of input neurons:     # gives only +outputs or 0:
  classifier.add(Dense(15, input_shape = (x_train.shape[1],), activation = 'relu'))
  classifier.add(Dropout(0.2)) # carring 80% of neurons to the next layer
  classifier.add(Dense(15, activation = 'relu')) # the rectified linear activation function or ReLu for short is a
  classifier.add(Dropout(0.4)) # taking 60% of neurons from this to the next layer

  # since this is a binary classification, the final depth should be 1.
  # the main reason wjy we use sigmoid function is because it exists betwenn (0 to 1)
  classifier.add(Dense(1, activation = 'sigmoid'))
  classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

  return classifier

# get the shape of the tensor
x_train.shape[1] # <-- the number of input neurons

# include bias:
(x_train.shape[1]+1)*15 # <-- the number of input neurons

model = model()
model.summary()

# Fit the training data into the model

# When adding validation_data to a model - it would
# train with every epoch on the train data and
# test with the validation data.
history = model.fit(
    x_train,
    y_train,
    validation_data=(x_test, y_test),
    epochs=5,
    verbose=1
)

"""Meaning:
- if val_accuracy increasing at every step, is the loss decreasing.
- if the accuracy really is increasing, is the validation accuracy also increasing? If this is not happening, the accuracy keeps on increasing, but the validation accuracy keeps on decreasing --> then this is highter variance in the model, the model is overfitting. So stop training and reset the model and change layers or alterations in the model, and check how well the data was pre-processed.

# **Evaluate the model**
"""

from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import precision_recall_curve, average_precision_score, f1_score, confusion_matrix

# function to plot the roc_curve
def plot_auc(t_y, p_y):
  fpr, tpr, thresholds = roc_curve(t_y, p_y, pos_label=1)
  fig, c_ax = plt.subplots(1,1, figsize = (9,9))
  c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)' %('classification', auc(fpr, tpr)))
  c_ax.plot([0,1],[0,1], color='navy', lw=1, linestyle='--')
  c_ax.legend()
  c_ax.set_xlabel('False Positive Rate')
  c_ax.set_ylabel('True Positive Rate')

# function to plot  the precision_recall_curve. Utilized precision_recall_curve imported above
def plot_precision_recall_curve_helper(t_y, p_y):
  fig, c_ax = plt.subplots(1,1, figsize = (9,9))
  precision, recall, thresholds = precision_recall_curve(t_y, p_y, pos_label=1)
  aps = average_precision_score(t_y, p_y)
  c_ax.plot(recall, precision, label = '%s (AP Score:%0.2f)' %('classification', aps))
  c_ax.plot(recall, precision, color='red', lw=2)
  c_ax.legend()
  c_ax.set_xlabel('Recall')
  c_ax.set_ylabel('Precision')

# function to plot the history
def plot_history(history):
  f = plt.figure()
  f.set_figwidth(15)

  f.add_subplot(1,2,1)
  plt.plot(history.history['val_loss'], label='val loss')
  plt.plot(history.history['loss'], label='train loss')
  plt.legend()
  plt.title('model loss')

  f.add_subplot(1,2,2)
  plt.plot(history.history['val_accuracy'], label='val accuracy')
  plt.plot(history.history['accuracy'], label='train aaccuracycc')
  plt.legend()

  plt.show()

hist = plot_history(history)

plot_auc(y_test, model.predict(x_test, verbose = True))

plot_precision_recall_curve_helper(y_test, model.predict(x_test, verbose = True))

"""Calculate f1 - score:"""

# Find the treshold that optimize the model's performance, and use that threshold to make binary classification.
# Consider all the metrics

def calc_f1(prec, recall):
  return 2*(prec*recall)/(prec+recall) if recall and prec else 0

precision, recall, thresholds = precision_recall_curve(y_test, model.predict(x_test, verbose = True))
f1_score = [calc_f1(precision[i], recall[i]) for i in range(len(thresholds))]
idx = np.argmax(f1_score)
threshold = thresholds[idx]
print('**************************************************************')
print('Precision: ' + str(precision[idx]))
print('Recall: ' + str(recall[idx]))
print('Threshold: ' + str(threshold))
print('F1 Score: ' + str(f1_score[idx]))

# the threshold or Cut-off represents in a binary classification the probability thta the prediction is true.
# It represents the
plt.figure()
plt.plot(thresholds, f1_score)
plt.title('F1 Score vs Threshold')
plt.xlabel('Threshold')
plt.ylabel('F1 Score')
plt.show()